{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_n = pd.read_csv(\"../neural_data/tripleN/filtered_units_by_roi_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>UnitID</th>\n",
       "      <th>ROI_Label</th>\n",
       "      <th>ROI_Index</th>\n",
       "      <th>Position</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>UnitType</th>\n",
       "      <th>Category</th>\n",
       "      <th>stim_1</th>\n",
       "      <th>stim_2</th>\n",
       "      <th>...</th>\n",
       "      <th>stim_1063</th>\n",
       "      <th>stim_1064</th>\n",
       "      <th>stim_1065</th>\n",
       "      <th>stim_1066</th>\n",
       "      <th>stim_1067</th>\n",
       "      <th>stim_1068</th>\n",
       "      <th>stim_1069</th>\n",
       "      <th>stim_1070</th>\n",
       "      <th>stim_1071</th>\n",
       "      <th>stim_1072</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1329.334106</td>\n",
       "      <td>0.896420</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4.617235</td>\n",
       "      <td>2.770339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.650426</td>\n",
       "      <td>8.680404</td>\n",
       "      <td>8.557277</td>\n",
       "      <td>0.646412</td>\n",
       "      <td>4.524891</td>\n",
       "      <td>23.732601</td>\n",
       "      <td>15.975641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1354.298706</td>\n",
       "      <td>0.569951</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>4.264396</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>...</td>\n",
       "      <td>2.132198</td>\n",
       "      <td>0.710732</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>1.150709</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>2.809085</td>\n",
       "      <td>7.648839</td>\n",
       "      <td>3.553664</td>\n",
       "      <td>2.132198</td>\n",
       "      <td>2.842931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1413.647705</td>\n",
       "      <td>0.970851</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>8.152885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>26.483892</td>\n",
       "      <td>10.957059</td>\n",
       "      <td>51.721512</td>\n",
       "      <td>36.454311</td>\n",
       "      <td>5.452566</td>\n",
       "      <td>91.706955</td>\n",
       "      <td>9.087609</td>\n",
       "      <td>27.626345</td>\n",
       "      <td>3.271540</td>\n",
       "      <td>2.388742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1422.297974</td>\n",
       "      <td>0.971530</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.922206</td>\n",
       "      <td>...</td>\n",
       "      <td>13.417530</td>\n",
       "      <td>2.367800</td>\n",
       "      <td>20.370598</td>\n",
       "      <td>13.304776</td>\n",
       "      <td>76.709221</td>\n",
       "      <td>1.578532</td>\n",
       "      <td>42.921082</td>\n",
       "      <td>6.314130</td>\n",
       "      <td>50.174809</td>\n",
       "      <td>8.494013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1477.774658</td>\n",
       "      <td>0.494290</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>2.473717</td>\n",
       "      <td>0.337325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.264470</td>\n",
       "      <td>0.562208</td>\n",
       "      <td>4.666333</td>\n",
       "      <td>9.669986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.891215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17385</th>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1946.013550</td>\n",
       "      <td>0.412192</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>2.286393</td>\n",
       "      <td>2.963844</td>\n",
       "      <td>...</td>\n",
       "      <td>2.935617</td>\n",
       "      <td>2.074689</td>\n",
       "      <td>2.074689</td>\n",
       "      <td>1.457222</td>\n",
       "      <td>0.592768</td>\n",
       "      <td>1.834759</td>\n",
       "      <td>0.938549</td>\n",
       "      <td>1.556017</td>\n",
       "      <td>0.518672</td>\n",
       "      <td>2.593364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17386</th>\n",
       "      <td>59</td>\n",
       "      <td>263</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1922.568237</td>\n",
       "      <td>0.661220</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>13.344816</td>\n",
       "      <td>5.994742</td>\n",
       "      <td>...</td>\n",
       "      <td>8.366575</td>\n",
       "      <td>11.060951</td>\n",
       "      <td>5.519072</td>\n",
       "      <td>12.680183</td>\n",
       "      <td>10.634151</td>\n",
       "      <td>6.098997</td>\n",
       "      <td>6.294476</td>\n",
       "      <td>6.613766</td>\n",
       "      <td>8.939979</td>\n",
       "      <td>2.873565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17387</th>\n",
       "      <td>59</td>\n",
       "      <td>266</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1972.743530</td>\n",
       "      <td>0.542228</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>0.592768</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.766254</td>\n",
       "      <td>2.593364</td>\n",
       "      <td>2.593362</td>\n",
       "      <td>2.371073</td>\n",
       "      <td>2.173484</td>\n",
       "      <td>1.037344</td>\n",
       "      <td>4.495163</td>\n",
       "      <td>1.556017</td>\n",
       "      <td>2.124088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17388</th>\n",
       "      <td>59</td>\n",
       "      <td>267</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1986.956299</td>\n",
       "      <td>0.730540</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>5.419603</td>\n",
       "      <td>4.742148</td>\n",
       "      <td>...</td>\n",
       "      <td>8.298759</td>\n",
       "      <td>4.001189</td>\n",
       "      <td>2.074690</td>\n",
       "      <td>2.074690</td>\n",
       "      <td>2.879163</td>\n",
       "      <td>2.963843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.185536</td>\n",
       "      <td>3.013241</td>\n",
       "      <td>2.790949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17389</th>\n",
       "      <td>59</td>\n",
       "      <td>269</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1975.089722</td>\n",
       "      <td>0.952837</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>2.945508</td>\n",
       "      <td>5.329968</td>\n",
       "      <td>...</td>\n",
       "      <td>6.381937</td>\n",
       "      <td>2.700048</td>\n",
       "      <td>5.338735</td>\n",
       "      <td>7.854693</td>\n",
       "      <td>5.400102</td>\n",
       "      <td>17.883461</td>\n",
       "      <td>14.175275</td>\n",
       "      <td>20.209414</td>\n",
       "      <td>12.457056</td>\n",
       "      <td>4.050074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17390 rows Ã— 1080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Session  UnitID ROI_Label  ROI_Index     Position  Reliability  \\\n",
       "0            1     136       MB1          3  1329.334106     0.896420   \n",
       "1            1     137       MB1          3  1354.298706     0.569951   \n",
       "2            1     139       MB1          3  1413.647705     0.970851   \n",
       "3            1     140       MB1          3  1422.297974     0.971530   \n",
       "4            1     143       MB1          3  1477.774658     0.494290   \n",
       "...        ...     ...       ...        ...          ...          ...   \n",
       "17385       59     258      AMC3         28  1946.013550     0.412192   \n",
       "17386       59     263      AMC3         28  1922.568237     0.661220   \n",
       "17387       59     266      AMC3         28  1972.743530     0.542228   \n",
       "17388       59     267      AMC3         28  1986.956299     0.730540   \n",
       "17389       59     269      AMC3         28  1975.089722     0.952837   \n",
       "\n",
       "       UnitType Category     stim_1    stim_2  ...  stim_1063  stim_1064  \\\n",
       "0             4        B   4.617235  2.770339  ...   0.000000  10.650426   \n",
       "1             2        B   4.264396  1.421464  ...   2.132198   0.710732   \n",
       "2             2        B   8.152885  0.000000  ...  26.483892  10.957059   \n",
       "3             4        B   0.000000  9.922206  ...  13.417530   2.367800   \n",
       "4             4        B   2.473717  0.337325  ...   0.730871   0.000000   \n",
       "...         ...      ...        ...       ...  ...        ...        ...   \n",
       "17385         2        F   2.286393  2.963844  ...   2.935617   2.074689   \n",
       "17386         2        F  13.344816  5.994742  ...   8.366575  11.060951   \n",
       "17387         2        F   0.592768  0.846812  ...   0.000000   2.766254   \n",
       "17388         2        F   5.419603  4.742148  ...   8.298759   4.001189   \n",
       "17389         1        F   2.945508  5.329968  ...   6.381937   2.700048   \n",
       "\n",
       "       stim_1065  stim_1066  stim_1067  stim_1068  stim_1069  stim_1070  \\\n",
       "0       8.680404   8.557277   0.646412   4.524891  23.732601  15.975641   \n",
       "1       1.421464   1.150709   1.421464   2.809085   7.648839   3.553664   \n",
       "2      51.721512  36.454311   5.452566  91.706955   9.087609  27.626345   \n",
       "3      20.370598  13.304776  76.709221   1.578532  42.921082   6.314130   \n",
       "4       0.000000   8.264470   0.562208   4.666333   9.669986   0.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "17385   2.074689   1.457222   0.592768   1.834759   0.938549   1.556017   \n",
       "17386   5.519072  12.680183  10.634151   6.098997   6.294476   6.613766   \n",
       "17387   2.593364   2.593362   2.371073   2.173484   1.037344   4.495163   \n",
       "17388   2.074690   2.074690   2.879163   2.963843   0.000000   1.185536   \n",
       "17389   5.338735   7.854693   5.400102  17.883461  14.175275  20.209414   \n",
       "\n",
       "       stim_1071  stim_1072  \n",
       "0       0.000000   0.861883  \n",
       "1       2.132198   2.842931  \n",
       "2       3.271540   2.388742  \n",
       "3      50.174809   8.494013  \n",
       "4       4.891215   0.000000  \n",
       "...          ...        ...  \n",
       "17385   0.518672   2.593364  \n",
       "17386   8.939979   2.873565  \n",
       "17387   1.556017   2.124088  \n",
       "17388   3.013241   2.790949  \n",
       "17389  12.457056   4.050074  \n",
       "\n",
       "[17390 rows x 1080 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "O    6658\n",
       "B    6121\n",
       "F    4611\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': array([[ 3.6832397 , 39.52490997, 34.94698715, ..., 10.82251453,\n",
       "          0.94681144,  4.87329531],\n",
       "        [ 6.62983179, 66.17266846, 61.16817856, ...,  4.06663752,\n",
       "         10.52631187,  9.46811771],\n",
       "        [24.30942726,  4.77542114,  4.73559809, ...,  3.80427575,\n",
       "          0.        ,  2.92397404],\n",
       "        ...,\n",
       "        [ 8.73453999, 22.65322495, 21.39789581, ...,  2.03332019,\n",
       "          0.11138958,  2.92397475],\n",
       "        [ 8.89239216, 60.05321121, 52.6929512 , ...,  7.21500397,\n",
       "          0.        ,  4.87329292],\n",
       "        [19.25812721, 44.83344269, 40.47185135, ...,  6.82145882,\n",
       "          0.        ,  3.89863515]], shape=(1000, 6658)),\n",
       " 'B': array([[ 1.49588633,  2.96611738,  0.        , ...,  0.        ,\n",
       "          3.45889449,  1.37741041],\n",
       "        [ 1.81643403,  0.67704797,  9.92220592, ...,  0.        ,\n",
       "          3.79057169,  0.        ],\n",
       "        [ 7.26573944,  9.80108547,  7.10339737, ...,  0.        ,\n",
       "          6.63350105,  2.3612752 ],\n",
       "        ...,\n",
       "        [12.78628254, 45.7169075 ,  2.55571961, ..., 11.33786583,\n",
       "          2.55863452,  0.        ],\n",
       "        [ 5.6986146 , 15.12074661,  6.31413221, ...,  0.        ,\n",
       "          1.73734474,  4.72255278],\n",
       "        [ 2.49314451,  0.        ,  4.5100956 , ...,  0.        ,\n",
       "          0.99502474,  0.        ]], shape=(1000, 6121)),\n",
       " 'F': array([[ 0.        ,  1.45401621, 37.76008224, ...,  4.73821163,\n",
       "         11.84554482,  2.62442207],\n",
       "        [ 0.        ,  0.77893734, 27.45738983, ...,  2.36910582,\n",
       "          3.31674767,  0.        ],\n",
       "        [ 0.        ,  0.98665404, 17.45046806, ...,  4.16962767,\n",
       "         10.93738556,  6.38715363],\n",
       "        ...,\n",
       "        [ 0.        ,  6.08869219,  2.07039261, ...,  0.99502474,\n",
       "          4.38284683,  8.44242573],\n",
       "        [ 0.        ,  9.90549469,  6.5562439 , ...,  2.46387005,\n",
       "          9.91077042,  4.11054087],\n",
       "        [ 0.        ,  1.09051239,  1.03519666, ...,  3.31674886,\n",
       "          7.30474758,  2.71928144]], shape=(1000, 4611))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n = triple_n.sort_values(by=\"Reliability\", ascending=False) #sort by reliability\n",
    "start_ind = int(np.where([\"stim_1\" == s for s in list(triple_n.columns)])[0][0])\n",
    "triple_n_FR = triple_n.iloc[:,start_ind:start_ind+1000].to_numpy()\n",
    "triple_n_FR = np.transpose(triple_n_FR)  \n",
    "\n",
    "cats = ['O', 'B', 'F']\n",
    "cat_units_FR = {}\n",
    "\n",
    "for cat in cats:\n",
    "    cat_units_indices = np.where(triple_n['Category'].values == cat)[0]\n",
    "    cat_FR = triple_n_FR[:, cat_units_indices]\n",
    "    cat_units_FR[cat] = cat_FR\n",
    "\n",
    "cat_units_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 116)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"/home/nirajesh/untrained_pred/neural_data/NSD_monkey/macaque_NSD_IT.npy\", allow_pickle=True)\n",
    "data.item()['response'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "vgg = torchvision.models.vgg16()\n",
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_layer_activations(model, layer, image_data, device_id=0):\n",
    "  # iterate through conv and linear layers\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "          activations.append(output.detach().cpu())\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for images in image_data:\n",
    "        images = images.to(f\"cuda:{device_id}\")\n",
    "        _ = model(images)\n",
    "        \n",
    "    handle.remove()\n",
    "    acts = torch.cat(activations, dim=0)\n",
    "    acts = acts.nan_to_num_(posinf=1e6, neginf=-1e6, nan=0.0)\n",
    "    max_val = torch.max(torch.abs(acts))\n",
    "\n",
    "    if len(acts.shape) > 2:\n",
    "      # avg each filter\n",
    "      acts = torch.nanmean(acts, dim=(-2, -1))\n",
    "      \n",
    "    acts = acts.nan_to_num_(posinf=1e6, neginf=-1e6, nan=0.0)\n",
    "\n",
    "\n",
    "    max_val = torch.max(torch.abs(acts))\n",
    "    \n",
    "    if max_val > 1e6 and max_val != 0:\n",
    "        # Normalize to keep within [-max_range, max_range]\n",
    "        scale = 1e6 / max_val\n",
    "        acts = acts * scale\n",
    "    \n",
    "    return acts.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "transform = models.VGG16_Weights.IMAGENET1K_V1.transforms()\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Lambda(lambda img: img.convert('RGB')),  \n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(\n",
    "#     #     mean=[0.485, 0.456, 0.406],\n",
    "#     #     std=[0.229, 0.224, 0.225]\n",
    "#     # ),\n",
    "# ])\n",
    "\n",
    "class NSD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.paths = sorted([os.path.join(folder, f) for f in os.listdir(folder) if (f.endswith(\".bmp\") and not f.startswith(\"MFO\"))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m vgg.named_modules():\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# layers: final 2 conv blocks and 2 fc layers\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m layer_names:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         X = \u001b[43mget_layer_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvgg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_layer_activations\u001b[39m\u001b[34m(model, layer, image_data, device_id)\u001b[39m\n\u001b[32m     11\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m image_data:\n\u001b[32m     12\u001b[39m     images = images.to(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m handle.remove()\n\u001b[32m     16\u001b[39m acts = torch.cat(activations, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torchvision/models/vgg.py:66\u001b[39m, in \u001b[36mVGG.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m     68\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/cogsci/home/nirajesh/untrained_pred/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "def get_NSD_dataset(folder):\n",
    "    dataset = NSD_Dataset(folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "    return dataloader\n",
    "image_data = get_NSD_dataset(\"/home/nirajesh/untrained_pred/neural_data/tripleN/images\")\n",
    "layer_names = ['features.23', 'features.30', 'classifier.0', 'classifier.3']\n",
    "for name, layer in vgg.named_modules():\n",
    "    # layers: final 2 conv blocks and 2 fc layers\n",
    "    if name in layer_names:\n",
    "        X = get_layer_activations(vgg, layer, image_data, 1)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
