{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_n = pd.read_csv(\"../neural_data/tripleN/filtered_units_by_roi_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>UnitID</th>\n",
       "      <th>ROI_Label</th>\n",
       "      <th>ROI_Index</th>\n",
       "      <th>Position</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>UnitType</th>\n",
       "      <th>Category</th>\n",
       "      <th>stim_1</th>\n",
       "      <th>stim_2</th>\n",
       "      <th>...</th>\n",
       "      <th>stim_1063</th>\n",
       "      <th>stim_1064</th>\n",
       "      <th>stim_1065</th>\n",
       "      <th>stim_1066</th>\n",
       "      <th>stim_1067</th>\n",
       "      <th>stim_1068</th>\n",
       "      <th>stim_1069</th>\n",
       "      <th>stim_1070</th>\n",
       "      <th>stim_1071</th>\n",
       "      <th>stim_1072</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1329.334106</td>\n",
       "      <td>0.896420</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4.617235</td>\n",
       "      <td>2.770339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.650426</td>\n",
       "      <td>8.680404</td>\n",
       "      <td>8.557277</td>\n",
       "      <td>0.646412</td>\n",
       "      <td>4.524891</td>\n",
       "      <td>23.732601</td>\n",
       "      <td>15.975641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1354.298706</td>\n",
       "      <td>0.569951</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>4.264396</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>...</td>\n",
       "      <td>2.132198</td>\n",
       "      <td>0.710732</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>1.150709</td>\n",
       "      <td>1.421464</td>\n",
       "      <td>2.809085</td>\n",
       "      <td>7.648839</td>\n",
       "      <td>3.553664</td>\n",
       "      <td>2.132198</td>\n",
       "      <td>2.842931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1413.647705</td>\n",
       "      <td>0.970851</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>8.152885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>26.483892</td>\n",
       "      <td>10.957059</td>\n",
       "      <td>51.721512</td>\n",
       "      <td>36.454311</td>\n",
       "      <td>5.452566</td>\n",
       "      <td>91.706955</td>\n",
       "      <td>9.087609</td>\n",
       "      <td>27.626345</td>\n",
       "      <td>3.271540</td>\n",
       "      <td>2.388742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1422.297974</td>\n",
       "      <td>0.971530</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.922206</td>\n",
       "      <td>...</td>\n",
       "      <td>13.417530</td>\n",
       "      <td>2.367800</td>\n",
       "      <td>20.370598</td>\n",
       "      <td>13.304776</td>\n",
       "      <td>76.709221</td>\n",
       "      <td>1.578532</td>\n",
       "      <td>42.921082</td>\n",
       "      <td>6.314130</td>\n",
       "      <td>50.174809</td>\n",
       "      <td>8.494013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>MB1</td>\n",
       "      <td>3</td>\n",
       "      <td>1477.774658</td>\n",
       "      <td>0.494290</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>2.473717</td>\n",
       "      <td>0.337325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.264470</td>\n",
       "      <td>0.562208</td>\n",
       "      <td>4.666333</td>\n",
       "      <td>9.669986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.891215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17385</th>\n",
       "      <td>59</td>\n",
       "      <td>258</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1946.013550</td>\n",
       "      <td>0.412192</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>2.286393</td>\n",
       "      <td>2.963844</td>\n",
       "      <td>...</td>\n",
       "      <td>2.935617</td>\n",
       "      <td>2.074689</td>\n",
       "      <td>2.074689</td>\n",
       "      <td>1.457222</td>\n",
       "      <td>0.592768</td>\n",
       "      <td>1.834759</td>\n",
       "      <td>0.938549</td>\n",
       "      <td>1.556017</td>\n",
       "      <td>0.518672</td>\n",
       "      <td>2.593364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17386</th>\n",
       "      <td>59</td>\n",
       "      <td>263</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1922.568237</td>\n",
       "      <td>0.661220</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>13.344816</td>\n",
       "      <td>5.994742</td>\n",
       "      <td>...</td>\n",
       "      <td>8.366575</td>\n",
       "      <td>11.060951</td>\n",
       "      <td>5.519072</td>\n",
       "      <td>12.680183</td>\n",
       "      <td>10.634151</td>\n",
       "      <td>6.098997</td>\n",
       "      <td>6.294476</td>\n",
       "      <td>6.613766</td>\n",
       "      <td>8.939979</td>\n",
       "      <td>2.873565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17387</th>\n",
       "      <td>59</td>\n",
       "      <td>266</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1972.743530</td>\n",
       "      <td>0.542228</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>0.592768</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.766254</td>\n",
       "      <td>2.593364</td>\n",
       "      <td>2.593362</td>\n",
       "      <td>2.371073</td>\n",
       "      <td>2.173484</td>\n",
       "      <td>1.037344</td>\n",
       "      <td>4.495163</td>\n",
       "      <td>1.556017</td>\n",
       "      <td>2.124088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17388</th>\n",
       "      <td>59</td>\n",
       "      <td>267</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1986.956299</td>\n",
       "      <td>0.730540</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>5.419603</td>\n",
       "      <td>4.742148</td>\n",
       "      <td>...</td>\n",
       "      <td>8.298759</td>\n",
       "      <td>4.001189</td>\n",
       "      <td>2.074690</td>\n",
       "      <td>2.074690</td>\n",
       "      <td>2.879163</td>\n",
       "      <td>2.963843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.185536</td>\n",
       "      <td>3.013241</td>\n",
       "      <td>2.790949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17389</th>\n",
       "      <td>59</td>\n",
       "      <td>269</td>\n",
       "      <td>AMC3</td>\n",
       "      <td>28</td>\n",
       "      <td>1975.089722</td>\n",
       "      <td>0.952837</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>2.945508</td>\n",
       "      <td>5.329968</td>\n",
       "      <td>...</td>\n",
       "      <td>6.381937</td>\n",
       "      <td>2.700048</td>\n",
       "      <td>5.338735</td>\n",
       "      <td>7.854693</td>\n",
       "      <td>5.400102</td>\n",
       "      <td>17.883461</td>\n",
       "      <td>14.175275</td>\n",
       "      <td>20.209414</td>\n",
       "      <td>12.457056</td>\n",
       "      <td>4.050074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17390 rows Ã— 1080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Session  UnitID ROI_Label  ROI_Index     Position  Reliability  \\\n",
       "0            1     136       MB1          3  1329.334106     0.896420   \n",
       "1            1     137       MB1          3  1354.298706     0.569951   \n",
       "2            1     139       MB1          3  1413.647705     0.970851   \n",
       "3            1     140       MB1          3  1422.297974     0.971530   \n",
       "4            1     143       MB1          3  1477.774658     0.494290   \n",
       "...        ...     ...       ...        ...          ...          ...   \n",
       "17385       59     258      AMC3         28  1946.013550     0.412192   \n",
       "17386       59     263      AMC3         28  1922.568237     0.661220   \n",
       "17387       59     266      AMC3         28  1972.743530     0.542228   \n",
       "17388       59     267      AMC3         28  1986.956299     0.730540   \n",
       "17389       59     269      AMC3         28  1975.089722     0.952837   \n",
       "\n",
       "       UnitType Category     stim_1    stim_2  ...  stim_1063  stim_1064  \\\n",
       "0             4        B   4.617235  2.770339  ...   0.000000  10.650426   \n",
       "1             2        B   4.264396  1.421464  ...   2.132198   0.710732   \n",
       "2             2        B   8.152885  0.000000  ...  26.483892  10.957059   \n",
       "3             4        B   0.000000  9.922206  ...  13.417530   2.367800   \n",
       "4             4        B   2.473717  0.337325  ...   0.730871   0.000000   \n",
       "...         ...      ...        ...       ...  ...        ...        ...   \n",
       "17385         2        F   2.286393  2.963844  ...   2.935617   2.074689   \n",
       "17386         2        F  13.344816  5.994742  ...   8.366575  11.060951   \n",
       "17387         2        F   0.592768  0.846812  ...   0.000000   2.766254   \n",
       "17388         2        F   5.419603  4.742148  ...   8.298759   4.001189   \n",
       "17389         1        F   2.945508  5.329968  ...   6.381937   2.700048   \n",
       "\n",
       "       stim_1065  stim_1066  stim_1067  stim_1068  stim_1069  stim_1070  \\\n",
       "0       8.680404   8.557277   0.646412   4.524891  23.732601  15.975641   \n",
       "1       1.421464   1.150709   1.421464   2.809085   7.648839   3.553664   \n",
       "2      51.721512  36.454311   5.452566  91.706955   9.087609  27.626345   \n",
       "3      20.370598  13.304776  76.709221   1.578532  42.921082   6.314130   \n",
       "4       0.000000   8.264470   0.562208   4.666333   9.669986   0.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "17385   2.074689   1.457222   0.592768   1.834759   0.938549   1.556017   \n",
       "17386   5.519072  12.680183  10.634151   6.098997   6.294476   6.613766   \n",
       "17387   2.593364   2.593362   2.371073   2.173484   1.037344   4.495163   \n",
       "17388   2.074690   2.074690   2.879163   2.963843   0.000000   1.185536   \n",
       "17389   5.338735   7.854693   5.400102  17.883461  14.175275  20.209414   \n",
       "\n",
       "       stim_1071  stim_1072  \n",
       "0       0.000000   0.861883  \n",
       "1       2.132198   2.842931  \n",
       "2       3.271540   2.388742  \n",
       "3      50.174809   8.494013  \n",
       "4       4.891215   0.000000  \n",
       "...          ...        ...  \n",
       "17385   0.518672   2.593364  \n",
       "17386   8.939979   2.873565  \n",
       "17387   1.556017   2.124088  \n",
       "17388   3.013241   2.790949  \n",
       "17389  12.457056   4.050074  \n",
       "\n",
       "[17390 rows x 1080 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "O    6658\n",
       "B    6121\n",
       "F    4611\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': array([[ 3.6832397 , 39.52490997, 34.94698715, ..., 10.82251453,\n",
       "          0.94681144,  4.87329531],\n",
       "        [ 6.62983179, 66.17266846, 61.16817856, ...,  4.06663752,\n",
       "         10.52631187,  9.46811771],\n",
       "        [24.30942726,  4.77542114,  4.73559809, ...,  3.80427575,\n",
       "          0.        ,  2.92397404],\n",
       "        ...,\n",
       "        [ 8.73453999, 22.65322495, 21.39789581, ...,  2.03332019,\n",
       "          0.11138958,  2.92397475],\n",
       "        [ 8.89239216, 60.05321121, 52.6929512 , ...,  7.21500397,\n",
       "          0.        ,  4.87329292],\n",
       "        [19.25812721, 44.83344269, 40.47185135, ...,  6.82145882,\n",
       "          0.        ,  3.89863515]], shape=(1000, 6658)),\n",
       " 'B': array([[ 1.49588633,  2.96611738,  0.        , ...,  0.        ,\n",
       "          3.45889449,  1.37741041],\n",
       "        [ 1.81643403,  0.67704797,  9.92220592, ...,  0.        ,\n",
       "          3.79057169,  0.        ],\n",
       "        [ 7.26573944,  9.80108547,  7.10339737, ...,  0.        ,\n",
       "          6.63350105,  2.3612752 ],\n",
       "        ...,\n",
       "        [12.78628254, 45.7169075 ,  2.55571961, ..., 11.33786583,\n",
       "          2.55863452,  0.        ],\n",
       "        [ 5.6986146 , 15.12074661,  6.31413221, ...,  0.        ,\n",
       "          1.73734474,  4.72255278],\n",
       "        [ 2.49314451,  0.        ,  4.5100956 , ...,  0.        ,\n",
       "          0.99502474,  0.        ]], shape=(1000, 6121)),\n",
       " 'F': array([[ 0.        ,  1.45401621, 37.76008224, ...,  4.73821163,\n",
       "         11.84554482,  2.62442207],\n",
       "        [ 0.        ,  0.77893734, 27.45738983, ...,  2.36910582,\n",
       "          3.31674767,  0.        ],\n",
       "        [ 0.        ,  0.98665404, 17.45046806, ...,  4.16962767,\n",
       "         10.93738556,  6.38715363],\n",
       "        ...,\n",
       "        [ 0.        ,  6.08869219,  2.07039261, ...,  0.99502474,\n",
       "          4.38284683,  8.44242573],\n",
       "        [ 0.        ,  9.90549469,  6.5562439 , ...,  2.46387005,\n",
       "          9.91077042,  4.11054087],\n",
       "        [ 0.        ,  1.09051239,  1.03519666, ...,  3.31674886,\n",
       "          7.30474758,  2.71928144]], shape=(1000, 4611))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_n = triple_n.sort_values(by=\"Reliability\", ascending=False) #sort by reliability\n",
    "start_ind = int(np.where([\"stim_1\" == s for s in list(triple_n.columns)])[0][0])\n",
    "triple_n_FR = triple_n.iloc[:,start_ind:start_ind+1000].to_numpy()\n",
    "triple_n_FR = np.transpose(triple_n_FR)  \n",
    "\n",
    "cats = ['O', 'B', 'F']\n",
    "cat_units_FR = {}\n",
    "\n",
    "for cat in cats:\n",
    "    cat_units_indices = np.where(triple_n['Category'].values == cat)[0]\n",
    "    cat_FR = triple_n_FR[:, cat_units_indices]\n",
    "    cat_units_FR[cat] = cat_FR\n",
    "\n",
    "cat_units_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coco_IDs': array([262145, 262239, 262414, 524646, 262690,    584,    605,    625,\n",
       "           650,   1308, 263599, 525790, 525932, 264244, 264396,   2270,\n",
       "          2372, 264735, 526968, 526980, 274108, 265023, 265150, 265453,\n",
       "        527618,   3521, 265745, 558788, 393907,   4442, 266622,   5028,\n",
       "        267338,   5277, 267647, 267699,   5684, 312536, 268008, 268114,\n",
       "        268418, 268659,   6765,   7048, 531392, 531515, 531828,  11299,\n",
       "        219750,   7932,   8027,   8053, 270278, 532753, 533231,   8998,\n",
       "          1625, 534259,  10046,  10710, 272901,  10903, 273120, 273138,\n",
       "        273147, 273250,  11358, 535871,  11856, 536786, 536884, 274978,\n",
       "        537393, 275663, 399878, 275902, 275938,  13969, 276664, 538822,\n",
       "        539340, 539395, 277524, 570543, 539705, 539879, 300383, 540538,\n",
       "         16314, 278555, 540735, 278714, 541077, 278962,  16898, 541258,\n",
       "        279197, 541472, 271183, 541856,  17967,  18078, 280484,  18367,\n",
       "        280764, 280808, 543254, 281330, 544060,  19863,  20517, 544926,\n",
       "        545160, 571485,  21095, 283426, 396793, 283678, 545950,  21718,\n",
       "        546140, 546147,  22256, 122964,  22964, 547367, 285450, 547644,\n",
       "         23392, 548593, 549301, 549830,  26029, 550396, 288372,  26388,\n",
       "         26443, 288751,  26702, 551164,  26981,  27002,  27030, 354070,\n",
       "         27298, 289460, 289791,  27665, 290192, 552589, 553150, 553166,\n",
       "        553586, 291560,  29730,  29732,  30109,  30148, 292845, 293130,\n",
       "        293368, 293372,  32134, 556542,  32275, 556748,  32809, 557848,\n",
       "        557886,  33727,  33764, 558822, 559816, 559145, 559225, 297154,\n",
       "        573830, 355458, 559958, 297844,  35927, 298170, 298395,   6053,\n",
       "         36589,  37358,  37779,  37823, 300881,  38801,  39196, 225054,\n",
       "        564465, 302588,  40685, 574784, 303392, 303436, 565652, 565834,\n",
       "         41606, 566201, 566261, 304187, 566341, 263276,  42578,  43150,\n",
       "         43176, 305685, 567837,  43776,  43815, 306031, 306249, 225829,\n",
       "        306988, 569196, 219735, 569645, 575701, 308950, 309135, 309192,\n",
       "         47172, 571499,  47638, 572215, 310870, 573337, 270325, 311232,\n",
       "        532520, 312003,  50256, 574760,  95809,  50624, 313480, 575971,\n",
       "        576011,  51774, 560396, 314379,  52348, 576749, 576789, 455691,\n",
       "        577817, 577964,  53778, 578169,  54001, 317061,  55184,   9199,\n",
       "         55206,  55294,  55402,  55514, 317781, 318381, 318444,   1877,\n",
       "        580813,  56837,  57631,  57796, 320533,  58407,  58465, 359337,\n",
       "         59321, 321902, 403184, 322008,   9983,   9987, 322246, 322255,\n",
       "        323010,  53842, 359687, 323182,  61174, 323397,  61266,  61288,\n",
       "         61559, 534553, 490875, 323970,  62547, 325710,  10743,  64534,\n",
       "         65012,  65029, 327191,  65400,  66253, 328454, 328628,  66696,\n",
       "         67347, 281575, 329641,  67623, 330084, 330086,  67961,  68238,\n",
       "         11373,  55090,  68623, 246728,  69625,  69705, 332261, 332448,\n",
       "        332498,  70421, 332574, 332692,   2349, 332875, 333152,  71429,\n",
       "        333615, 333629, 333654, 579906, 334011, 334048,  71927, 334208,\n",
       "         72095, 334772,  72715, 335063, 492851, 335688, 335824,  73997,\n",
       "        281567, 336374,  74390,  74461,  74714,  74828, 337479, 274776,\n",
       "        338169,  76476, 338670,  12795, 338987,  77870,  78000,  78042,\n",
       "        340317, 340734,  79111, 275335,  80084,  80275,  80619,   8471,\n",
       "         80987,  81105,  81365,  81474,  81841,  46372,  81961,  81995,\n",
       "        450584, 319517, 569678,  82650,  82770, 345149,  83393, 345725,\n",
       "        345751,  83632,  83738,  84174, 346583, 346754, 346795, 348204,\n",
       "         86811,  87126,  87146, 349363,  87334,  87871, 320508, 350299,\n",
       "         88414,  88835,  89908, 352129, 189765, 352236, 352724, 352861,\n",
       "         90827,  90994,  91005, 408492, 364806,  91784, 354045, 354095,\n",
       "         92001,  92221,  92288,  92353, 354761,  59194, 355622, 277778,\n",
       "        356771, 357074, 357228,  95176,  95337, 357865,  95828,  96161,\n",
       "        359085,  97201,  97218,  98480,  60117, 147547, 361193, 361197,\n",
       "         99363, 361648,  99580, 100396, 362677, 100599, 100863, 101069,\n",
       "        101952, 570785, 365003, 365218, 103366, 366146, 104044, 104081,\n",
       "        366396, 104502, 366802, 367552, 106107, 368852, 369171, 369230,\n",
       "        369253, 107204,  17935, 542304, 370369, 110611, 372775, 110780,\n",
       "        372959, 373679, 373970, 112734, 113593, 114035, 114077, 376407,\n",
       "        114274, 376643, 376858, 114822, 377576, 377703, 115637, 115649,\n",
       "        115967, 116588, 116603, 150636, 379713, 380227, 380729, 380989,\n",
       "        282032, 119669, 575164, 381961, 353175, 120655, 382890, 382951,\n",
       "        120987, 383641, 383652, 413472, 384235, 544738, 122776, 122811,\n",
       "        282631, 282707, 123692, 385864, 123891, 124711, 124747, 125042,\n",
       "        387318, 125586, 125703, 125725, 388980, 389582, 390120, 239828,\n",
       "        129060, 391678, 391754, 130181, 392556, 130681, 392851, 196584,\n",
       "        371353, 130973, 131011,  21826, 131524, 284089, 131763, 394008,\n",
       "        131967, 133281, 395758, 395775, 197068,  66014, 134169, 396550,\n",
       "        134562, 135032, 397192, 135316,  22627, 398083, 135965, 136111,\n",
       "        136142,  22696, 398540, 399162, 197656, 399835, 137706, 399984,\n",
       "        138153, 400581, 229233, 138785, 138904, 139344, 139551, 401699,\n",
       "        139561, 401720, 139815, 402057, 402199, 140590, 198195, 285588,\n",
       "        140724, 154556, 403535, 403739, 403789, 404007, 404282, 504289,\n",
       "        196777, 404823, 142803, 405215, 143245, 405437, 388853, 143420,\n",
       "        143594, 406292, 144193, 406445, 406595, 407778, 461187,  24287,\n",
       "        504888, 146256, 408922, 408965, 147331, 147694, 148263, 149221,\n",
       "        411475, 149791, 149810, 153394, 150196, 150559, 412855, 412857,\n",
       "        369367, 412922, 150931, 151163, 151307, 462134, 151414,  68996,\n",
       "        414176, 152273, 152328, 549729, 506039, 414853, 415026, 162230,\n",
       "        415608, 154011, 416279, 154207, 416972, 417141, 417264, 417700,\n",
       "        418115, 419212, 156084, 113449, 157119, 157125, 157181, 157342,\n",
       "        419586, 563961, 159680, 422268, 160142, 160181, 463620, 551063,\n",
       "        160735, 161062, 423209, 423222,  14269, 428726,  43829, 431764,\n",
       "        431825, 284987,  28367, 433019, 433021, 170917, 170968,  79701,\n",
       "        171328, 171374, 420713,  28930, 435902, 173987, 174213, 174303,\n",
       "        436578, 174504, 174522, 291280, 437516, 175715, 437967, 438154,\n",
       "        176085, 438751, 176728, 176873,  73172, 439241, 178031, 440334,\n",
       "        178423, 440792, 395740, 179019, 441172, 179034, 179480, 179914,\n",
       "        442489, 180410, 180609, 443377, 204969, 443575, 161354, 443940,\n",
       "        181837, 189388, 182799, 445351, 445726, 183719, 184822, 447130,\n",
       "        447182, 185225,  36151,   6178, 185888,  55764, 448401, 442666,\n",
       "        186828, 449914, 276853, 188225, 188660, 450864, 450903, 450914,\n",
       "        451680, 189794, 189888, 189893, 452341, 190271, 190487, 303133,\n",
       "        191005, 453221, 468771, 453682, 191686, 191740, 192576, 192631,\n",
       "        454830, 454915, 455020, 119550, 193023, 193318, 193373, 455657,\n",
       "        455799, 194054, 194108, 338439, 195650, 195768, 338501, 458178,\n",
       "        196365, 196368, 458616, 459190, 197118, 197401, 460236, 460408,\n",
       "        198319, 397815, 461428, 462505, 200391, 201722, 201758, 557920,\n",
       "        201969, 202050, 464616, 202805, 465412, 298139, 203937,  77686,\n",
       "        466265, 204422, 204806, 467646, 205875, 296474, 468394, 206467,\n",
       "        468825, 469445, 203615, 207549, 469816, 129143, 470907, 208805,\n",
       "        208849, 209229, 471871,  34961, 209919, 210065, 341033, 211198,\n",
       "        474024, 207833, 474353, 212229, 212380, 474545, 212440, 474827,\n",
       "        475236, 122923, 213288, 213988, 214059, 476380, 476481, 560054,\n",
       "        215023, 215291, 385417,  35972, 216029, 479035, 444880, 294744,\n",
       "        480601, 480990, 429887, 429902, 561006, 482858, 482912, 342639,\n",
       "        167908, 221441, 484300, 222921, 485113, 223447, 485605, 223718,\n",
       "        386856, 561677, 486503, 224396, 224759, 224869, 225113, 225383,\n",
       "        225533, 487825, 225750, 430850, 488406, 168803, 488658,   7567,\n",
       "        227307, 227451, 227733, 227843, 227928, 228133, 228316, 228380,\n",
       "        228565, 229456, 491611, 356810, 492037, 229947,  38370, 354185,\n",
       "        230422, 492627, 300622, 231102, 235479, 231851, 232277, 494552,\n",
       "        232648, 495040, 233104, 495387, 344729, 495568, 233580, 233873,\n",
       "        497464,  82945, 497797, 498615, 237093, 499627, 499730, 237697,\n",
       "        238001, 238201, 239337, 501739, 501867, 501926, 502058, 240278,\n",
       "        502470, 214821, 502860, 241026,  40187, 112805, 242133, 242631,\n",
       "        433657, 505486, 505592, 505643, 505661, 505738, 243839, 244334,\n",
       "        244476, 244578, 244967, 434069, 507642, 507789, 477852, 245895,\n",
       "        246166, 246453, 508612, 246759, 247121, 509292, 247893, 510334,\n",
       "        510900, 303637, 511127, 511165, 216378, 249905, 251084, 304092,\n",
       "        251818, 252160, 514563, 514955, 515102, 515296, 515508, 254130,\n",
       "        516634, 304627, 517878]),\n",
       " 'response': array([[19.96927803, 34.86943164, 13.51766513, ...,  8.44854071,\n",
       "         28.57142857,  4.60829493],\n",
       "        [34.56221198, 44.39324117, 24.11674347, ...,  7.52688172,\n",
       "          8.44854071,  5.99078341],\n",
       "        [12.13517665, 17.20430108, 20.73732719, ...,  6.9124424 ,\n",
       "         12.13517665,  4.91551459],\n",
       "        ...,\n",
       "        [ 2.4577573 , 20.89093702, 15.66820276, ...,  7.21966206,\n",
       "         71.88940092,  8.29493088],\n",
       "        [14.90015361, 19.35483871, 14.90015361, ..., 10.29185868,\n",
       "         27.49615975,  3.22580645],\n",
       "        [ 9.98463902, 18.74039939, 16.12903226, ...,  6.9124424 ,\n",
       "         18.27956989,  3.22580645]], shape=(979, 116))}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"/home/nirajesh/untrained_pred/neural_data/NSD_monkey/macaque_NSD_IT.npy\", allow_pickle=True)\n",
    "data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputWrapper(\n",
       "  (input_layer): Identity()\n",
       "  (model): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "vgg = torchvision.models.vgg16()\n",
    "class InputWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Identity()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        return self.model(x)\n",
    "\n",
    "new_vgg = InputWrapper(vgg)\n",
    "new_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "transform = models.VGG16_Weights.IMAGENET1K_V1.transforms()\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Lambda(lambda img: img.convert('RGB')),  \n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(\n",
    "#     #     mean=[0.485, 0.456, 0.406],\n",
    "#     #     std=[0.229, 0.224, 0.225]\n",
    "#     # ),\n",
    "# ])\n",
    "\n",
    "class NSD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.paths = sorted([os.path.join(folder, f) for f in os.listdir(folder) if (f.endswith(\".bmp\") and not f.startswith(\"MFO\"))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def get_layer_activations(model, layer, image_data, device_id=0):\n",
    "  # iterate through conv and linear layers\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "          activations.append(output.detach().cpu())\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for images in image_data:\n",
    "        images = images.to(f\"cuda:{device_id}\")\n",
    "        _ = model(images)\n",
    "        \n",
    "    handle.remove()\n",
    "    acts = torch.cat(activations, dim=0)\n",
    "    acts = acts.nan_to_num_(posinf=1e6, neginf=-1e6, nan=0.0)\n",
    "    print(acts.shape)\n",
    "    max_val = torch.max(torch.abs(acts))\n",
    "    target_dim = 4096\n",
    "    \n",
    "    if len(acts.shape) > 2:\n",
    "      # avg each filter\n",
    "      # acts = torch.nanmean(acts, dim=(-2, -1))\n",
    "        n_channels = acts.shape[1]\n",
    "        pool_dim = int(np.round(np.sqrt(target_dim/n_channels)))\n",
    "        print(pool_dim)\n",
    "        apool = nn.AdaptiveAvgPool2d((pool_dim, pool_dim))\n",
    "        acts = apool(acts)\n",
    "        acts = acts.flatten(1)\n",
    "\n",
    "    print(acts.shape)\n",
    "    \n",
    "      \n",
    "    acts = acts.nan_to_num_(posinf=1e6, neginf=-1e6, nan=0.0)\n",
    "\n",
    "\n",
    "    max_val = torch.max(torch.abs(acts))\n",
    "    \n",
    "    if max_val > 1e6 and max_val != 0:\n",
    "        # Normalize to keep within [-max_range, max_range]\n",
    "        scale = 1e6 / max_val\n",
    "        acts = acts * scale\n",
    "    \n",
    "    return acts.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 224, 224])\n",
      "37\n",
      "torch.Size([1000, 4107])\n"
     ]
    }
   ],
   "source": [
    "def get_NSD_dataset(folder):\n",
    "    dataset = NSD_Dataset(folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "    return dataloader\n",
    "    \n",
    "image_data = get_NSD_dataset(\"/home/nirajesh/untrained_pred/neural_data/tripleN/images\")\n",
    "layer_names = ['input_layer', 'features.23', 'features.30', 'classifier.0', 'classifier.3']\n",
    "vgg= new_vgg.to(f\"cuda:{1}\")\n",
    "for name, layer in vgg.named_modules():\n",
    "    # layers: final 2 conv blocks and 2 fc layers\n",
    "    if name in layer_names:\n",
    "        X = get_layer_activations(vgg, layer, image_data, 1)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
